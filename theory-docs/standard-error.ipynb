{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f2c45f",
   "metadata": {},
   "source": [
    "# Standard Error (SE)\n",
    "\n",
    "The **Standard Error (SE)** measures **how much your estimate would vary** if you repeated the same experiment or sampling process over and over again.\n",
    "\n",
    "Why is Standard Error Important?\n",
    "\n",
    "- It tells you how precise your estimate is. A small SE means your estimate is stable and trustworthy.\n",
    "- It’s used to build **confidence intervals**.\n",
    "- It helps with hypothesis testing. You use SE to compute **t-values** and **p-values** to test if a coefficient is significantly different from 0.\n",
    "\n",
    "---\n",
    "\n",
    "## Standard Error for $\\hat{\\beta}_1$, ${SE}(\\hat{\\beta}_1)$\n",
    "\n",
    "In a simple linear regression model, we have:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\text{i.i.d. } N(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "We aim to derive the **standard error** of the OLS estimator $\\hat{\\beta}_1$, which is:\n",
    "\n",
    "$$\n",
    "\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\mathrm{Var}(\\hat{\\beta}_1)}\n",
    "$$\n",
    "\n",
    "> Note: $\\text{SE}(\\bar{x}) = \\frac{s}{\\sqrt{n}}$ is the **sample-based formula** for standard error of the sample mean. $\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\mathrm{Var}(\\hat{\\beta}_1)}$ is an **estimate of the true (population-based)** version\n",
    "\n",
    "**Step 1: Formula for $\\beta_1$**\n",
    "\n",
    "The OLS estimator for the slope $\\beta_1$ is:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{ \\sum (x_i - \\bar{x})(y_i - \\bar{y}) }{ \\sum (x_i - \\bar{x})^2 }\n",
    "$$\n",
    "\n",
    "We now substitute the model expression for $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n",
    "\n",
    "To do this correctly, we also compute $\\bar{y}$, the sample mean of all $y_i$'s:\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{1}{n} \\sum y_i = \\frac{1}{n} \\sum \\left( \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\right)\n",
    "= \\beta_0 + \\beta_1 \\bar{x} + \\bar{\\varepsilon}\n",
    "$$\n",
    "\n",
    "Thus, the difference $y_i - \\bar{y}$ becomes:\n",
    "\n",
    "$$\n",
    "y_i - \\bar{y} = \\left( \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\right) - \\left( \\beta_0 + \\beta_1 \\bar{x} + \\bar{\\varepsilon} \\right)\n",
    "= \\beta_1(x_i - \\bar{x}) + \\varepsilon_i - \\bar{\\varepsilon}\n",
    "$$\n",
    "\n",
    "Now plug the expression into the OLS formula:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{ \\sum (x_i - \\bar{x}) \\left[ \\beta_1(x_i - \\bar{x}) + \\varepsilon_i - \\bar{\\varepsilon} \\right] }{ \\sum (x_i - \\bar{x})^2 }\n",
    "$$\n",
    "\n",
    "Expand the numerator using linearity of summation:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{\\beta_1 \\sum (x_i - \\bar{x})^2 + \\sum (x_i - \\bar{x}) \\varepsilon_i - \\sum (x_i - \\bar{x}) \\bar{\\varepsilon}}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "Note that $\\sum (x_i - \\bar{x}) = 0$, so:\n",
    "\n",
    "$$\n",
    "\\sum (x_i - \\bar{x}) \\bar{\\varepsilon} = \\bar{\\varepsilon} \\cdot \\sum (x_i - \\bar{x}) = 0\n",
    "$$\n",
    "\n",
    "Thus the expression simplifies to:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\beta_1 + \\frac{ \\sum (x_i - \\bar{x}) \\varepsilon_i }{ \\sum (x_i - \\bar{x})^2 }\n",
    "$$\n",
    "\n",
    "Let $S_{xx} = \\sum (x_i - \\bar{x})^2$, then:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\beta_1 + \\frac{ \\sum a_i \\varepsilon_i }{ S_{xx} }\n",
    "$$\n",
    "\n",
    "where $a_i = x_i - \\bar{x}$\n",
    "\n",
    "**Step 2: Compute the Variance of $\\beta_1$**\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{\\beta}_1) = \\mathrm{Var} \\left( \\frac{ \\sum a_i \\varepsilon_i }{ S_{xx} } \\right)\n",
    "= \\frac{1}{S_{xx}^2} \\cdot \\mathrm{Var} \\left( \\sum a_i \\varepsilon_i \\right)\n",
    "$$\n",
    "\n",
    "Since $\\varepsilon_i \\sim \\text{i.i.d.} \\text{ and } \\mathrm{Var}(\\varepsilon_i) = \\sigma^2\\text{:}$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var} \\left( \\sum a_i \\varepsilon_i \\right)\n",
    "= \\sum a_i^2 \\cdot \\operatorname{Var}(\\varepsilon_i) = \\sigma^2 \\sum a_i^2 = \\sigma^2 S_{xx}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{\\beta}_1) = \\frac{ \\sigma^2 S_{xx} }{ S_{xx}^2 } = \\frac{ \\sigma^2 }{ S_{xx} }\n",
    "$$\n",
    "\n",
    "**Step 3: Final Standard Error Formula**\n",
    "\n",
    "$$\n",
    "\\text{SE}(\\hat{\\beta}_1) = \\sqrt{ \\frac{ \\sigma^2 }{ \\sum (x_i - \\bar{x})^2 } }\n",
    "$$\n",
    "\n",
    "Since $\\sigma^2$ is unknown in real-world data, we estimate it using Residual Standard Error, RSE:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n - 2} \\sum (y_i - \\hat{y}_i)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "Therefore, the estimated standard error becomes:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\text{SE}(\\hat{\\beta}_1) = \\sqrt{ \\frac{ \\hat{\\sigma}^2 }{ \\sum (x_i - \\bar{x})^2 } }\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Standard Error for $\\hat{\\beta}_0$, ${SE}(\\hat{\\beta}_0)$\n",
    "\n",
    "**Step 1: OLS Formula for $\\beta_0$**\n",
    "\n",
    "The OLS estimator for the intercept is:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "This formula comes from rearranging the regression line to pass through the point $ (\\bar{x}, \\bar{y}) $, which is a property of the least squares line.\n",
    "\n",
    "**Step 2: Compute the Variance of $\\beta_0$**\n",
    "\n",
    "We apply the variance formula for a linear combination of random variables:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{\\beta}_0) = \\mathrm{Var}(\\bar{y} - \\hat{\\beta}_1 \\bar{x})\n",
    "$$\n",
    "\n",
    "Since $\\bar{x}$ is constant (it's computed from the observed values), we treat it as a scalar:\n",
    "\n",
    "$$\n",
    "= \\mathrm{Var}(\\bar{y}) + \\bar{x}^2 \\cdot \\mathrm{Var}(\\hat{\\beta}_1)\n",
    "- 2 \\cdot \\bar{x} \\cdot \\mathrm{Cov}(\\bar{y}, \\hat{\\beta}_1)\n",
    "$$\n",
    "\n",
    "**Step 3: Evaluate Each Term**\n",
    "\n",
    "1. Variance of $\\bar{y}$\n",
    "\n",
    "We know that the sample mean is defined as:\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$$\n",
    "\n",
    "From the regression model $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, we see that the randomness in $y_i$ comes from the error term $\\varepsilon_i$. Since $\\beta_0$, $\\beta_1$, and $x_i$ are fixed values, the only random part of $\\bar{y}$ is the average of the errors:\n",
    "\n",
    "$$\\bar{y} = \\beta_0 + \\beta_1 \\bar{x} + \\bar{\\varepsilon}, \\quad \\text{where } \\bar{\\varepsilon} = \\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_i$$\n",
    "\n",
    "Therefore, the variance of $\\bar{y}$ is equal to the variance of $\\bar{\\varepsilon}$. Because the $\\varepsilon_i$’s are independent and identically distributed with variance $\\sigma^2$, we use a standard result from probability:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\bar{\\varepsilon}) = \\mathrm{Var} \\left( \\frac{1}{n} \\sum_{i=1}^n \\varepsilon_i \\right)\n",
    "= \\frac{1}{n^2} \\sum_{i=1}^n \\mathrm{Var}(\\varepsilon_i)\n",
    "= \\frac{1}{n^2} \\cdot n \\cdot \\sigma^2\n",
    "= \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\bar{y}) = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "2. Variance of $\\hat{\\beta}_1$\n",
    "\n",
    "Previously derived as:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "Let $S_{xx} = \\sum (x_i - \\bar{x})^2$\n",
    "\n",
    "3. Covariance Term\n",
    "\n",
    "Under the assumption that $\\varepsilon_i$ are i.i.d. and independent of $x_i$, it can be shown that:\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(\\bar{y}, \\hat{\\beta}_1) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "**Step 4: Combine the Results**\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}_0)\n",
    "= \\frac{\\sigma^2}{n} + \\bar{x}^2 \\cdot \\frac{\\sigma^2}{S_{xx}}\n",
    "= \\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right)\n",
    "$$\n",
    "\n",
    "**Step 5: Final Standard Error Formula**\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\text{SE}(\\hat{\\beta}_0) = \\sqrt{ \\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2} \\right) }\n",
    "}\n",
    "$$\n",
    "\n",
    "Since $\\sigma^2$ is unknown in practice, it is estimated by:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n - 2} \\sum (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Therefore, the estimated standard error is:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\text{SE}(\\hat{\\beta}_0) = \\sqrt{ \\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2} \\right) }\n",
    "}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
