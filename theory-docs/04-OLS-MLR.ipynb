{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d607cd2",
   "metadata": {},
   "source": [
    "# Derivation of the Ordinary Least Squares Estimator for Multiple Regression\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i.$\n",
    "\n",
    "where\n",
    "- $\\beta_0$: is the interception\n",
    "- $\\varepsilon_i$: Error term\n",
    "\n",
    "## 1. Model setup\n",
    "\n",
    "We have $n$ observations and $p$ predictors. In vector/matrix form:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{pmatrix},\\quad\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "1 & x_{11} & \\dots & x_{1p}\\\\\n",
    "1 & x_{21} & \\dots & x_{2p}\\\\\n",
    "\\vdots & \\vdots &      & \\vdots\\\\\n",
    "1 & x_{n1} & \\dots & x_{np}\n",
    "\\end{pmatrix},\\quad\n",
    "\\beta =\n",
    "\\begin{pmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_p\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "so that our fitted values are $\\hat y = X\\beta$ and the residuals are $r = y - X\\beta$.\n",
    "\n",
    "> Why a column of 1’s?\n",
    "\n",
    "In multiple regression we write our predictions as  \n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}$ \n",
    "\n",
    "To capture the constant term $\\beta_0$ in matrix form, we prepend a column of 1’s to $X$:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "\\color{gray}{1} & x_{11} & \\dots & x_{1p}\\\\\n",
    "\\color{gray}{1} & x_{21} & \\dots & x_{2p}\\\\\n",
    "\\vdots         & \\vdots &      & \\vdots\\\\\n",
    "\\color{gray}{1} & x_{n1} & \\dots & x_{np}\n",
    "\\end{pmatrix},\\quad\n",
    "\\beta =\n",
    "\\begin{pmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_p\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Ordinary least squares criterion\n",
    "\n",
    "We choose $\\hat\\beta$ to minimize the sum of squared residuals:\n",
    "\n",
    "$$\n",
    "S(\\beta)\n",
    "= (y - X\\beta)^\\top\\,(y - X\\beta)\n",
    "= \\sum_{i=1}^n \\bigl(y_i - x_i^\\top\\beta\\bigr)^2.\n",
    "$$\n",
    "\n",
    "### Detailed proof\n",
    "\n",
    "1. **Define the residual vector**  \n",
    "   \n",
    "   Let  \n",
    "\n",
    "   $r = y - X\\beta = \\begin{pmatrix} r_1 \\\\ \\vdots \\\\ r_n \\end{pmatrix}$\n",
    "\n",
    "   where each residual is \n",
    "\n",
    "   $$r_i = y_i - x_i^\\top\\beta$$\n",
    "\n",
    "2. **Rewrite in dot‐product form**  \n",
    "   By definition of the transpose,\n",
    "   $$\n",
    "   (y - X\\beta)^\\top(y - X\\beta) = r^\\top r.\n",
    "   $$\n",
    "\n",
    "3. **Expand the dot‐product**  ($\\sum_{i=1}^n r_i^2$ is sum of residual square)\n",
    "   \n",
    "   For any vector $r$,  \n",
    "   $$\n",
    "   r^\\top r\n",
    "   = \\sum_{i=1}^n r_i \\, r_i\n",
    "   = \\sum_{i=1}^n r_i^2.\n",
    "   $$\n",
    "\n",
    "4. **Substitute the residuals**  \n",
    "   Since $r_i = y_i - x_i^\\top\\beta$,  \n",
    "   $$\n",
    "   r^\\top r\n",
    "   = \\sum_{i=1}^n (y_i - x_i^\\top\\beta)^2.\n",
    "   $$\n",
    "\n",
    "Putting it all together,  \n",
    "$$\n",
    "S(\\beta)\n",
    "= (y - X\\beta)^\\top(y - X\\beta)\n",
    "= \\sum_{i=1}^n \\bigl(y_i - x_i^\\top\\beta\\bigr)^2,\n",
    "$$  \n",
    "as required.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let’s take $n=2$ observations with\n",
    "\n",
    "$$\n",
    "y = \\begin{pmatrix}3\\\\5\\end{pmatrix},\\quad\n",
    "X\\beta = \\begin{pmatrix}2\\\\4\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "1. **Compute the residual vector**  \n",
    "   $$\n",
    "   r = y - X\\beta\n",
    "     = \\begin{pmatrix}3\\\\5\\end{pmatrix}\n",
    "       - \\begin{pmatrix}2\\\\4\\end{pmatrix}\n",
    "     = \\begin{pmatrix}1\\\\1\\end{pmatrix}.\n",
    "   $$\n",
    "\n",
    "2. **Form the dot‐product**  \n",
    "   $$\n",
    "   r^\\top r\n",
    "   = \\begin{pmatrix}1 & 1\\end{pmatrix}\n",
    "     \\begin{pmatrix}1\\\\1\\end{pmatrix}\n",
    "   = 1\\cdot1 + 1\\cdot1\n",
    "   = 2.\n",
    "   $$\n",
    "\n",
    "3. **Compare to the sum of squares**  \n",
    "   $$\n",
    "   \\sum_{i=1}^2 r_i^2\n",
    "   = 1^2 + 1^2\n",
    "   = 2.\n",
    "   $$\n",
    "\n",
    "Hence in this toy example,\n",
    "\n",
    "$$\n",
    "(y - X\\beta)^\\top(y - X\\beta)\n",
    "= r^\\top r\n",
    "= \\sum_{i=1}^2 r_i^2,\n",
    "$$\n",
    "\n",
    "demonstrating step 2.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Expand $S(\\beta)$\n",
    "\n",
    "Using $(A - B)^\\top = A^\\top - B^\\top$ and $(AB)^\\top = B^\\top A^\\top$, we get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "S(\\beta)\n",
    "&= (y - X\\beta)^\\top(y - X\\beta) \\\\\n",
    "&= y^\\top y\n",
    "  - y^\\top(X\\beta)\n",
    "  - (X\\beta)^\\top y\n",
    "  + (X\\beta)^\\top(X\\beta) \\\\\n",
    "&= y^\\top y\n",
    "  - \\beta^\\top X^\\top y\n",
    "  - \\beta^\\top X^\\top y\n",
    "  + \\beta^\\top X^\\top X\\,\\beta \\\\\n",
    "&= y^\\top y\n",
    "  - 2\\,\\beta^\\top X^\\top y\n",
    "  + \\beta^\\top (X^\\top X)\\,\\beta.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Compute the gradient $\\nabla_\\beta S$\n",
    "\n",
    "- $\\displaystyle \\frac{\\partial}{\\partial\\beta}\\!\\bigl(c^\\top\\beta\\bigr) = c$  \n",
    "- $\\displaystyle \\frac{\\partial}{\\partial\\beta}\\!\\bigl(\\beta^\\top A\\beta\\bigr) = (A + A^\\top)\\beta$, and since $X^\\top X$ is symmetric this is $2X^\\top X\\,\\beta$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\beta S\n",
    "&= \\frac{\\partial}{\\partial\\beta}\\bigl[y^\\top y\\bigr]\n",
    "  + \\frac{\\partial}{\\partial\\beta}\\bigl(-2\\,\\beta^\\top X^\\top y\\bigr)\n",
    "  + \\frac{\\partial}{\\partial\\beta}\\bigl(\\beta^\\top X^\\top X\\,\\beta\\bigr) \\\\\n",
    "&= 0 \\;-\\;2\\,X^\\top y \\;+\\;2\\,X^\\top X\\,\\beta.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Detailed proof\n",
    "\n",
    "We start from the expanded form:  \n",
    "$$\n",
    "S(\\beta)\n",
    "= y^\\top y\n",
    "\\;-\\;2\\,\\beta^\\top X^\\top y\n",
    "\\;+\\;\\beta^\\top X^\\top X\\,\\beta.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "1. **Derivative of the constant term**  \n",
    "   Since $y^\\top y$ does not depend on $\\beta$,  \n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial\\beta}\\bigl(y^\\top y\\bigr)\n",
    "   = 0.\n",
    "   $$\n",
    "\n",
    "2. **Derivative of the linear term**  \n",
    "   Write  \n",
    "   $$\n",
    "   -2\\,\\beta^\\top X^\\top y\n",
    "   = -2\\,(X^\\top y)^\\top \\beta\n",
    "   = -2\\,c^\\top \\beta,\n",
    "   \\quad c = X^\\top y.\n",
    "   $$  \n",
    "   A standard result gives  \n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial\\beta}\\!\\bigl(c^\\top \\beta\\bigr)\n",
    "   = c.\n",
    "   $$  \n",
    "   Therefore  \n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial\\beta}\\!\\bigl(-2\\,\\beta^\\top X^\\top y\\bigr)\n",
    "   = -2\\,c\n",
    "   = -2\\,X^\\top y.\n",
    "   $$\n",
    "\n",
    "3. **Derivative of the quadratic term**  \n",
    "   Let $A = X^\\top X$.  The term is  \n",
    "   $$\n",
    "   \\beta^\\top A\\,\\beta.\n",
    "   $$  \n",
    "   By matrix‐calculus,  \n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial\\beta}\\!\\bigl(\\beta^\\top A\\,\\beta\\bigr)\n",
    "   = (A + A^\\top)\\,\\beta.\n",
    "   $$  \n",
    "   Since $A$ is symmetric, by definition $A = A^\\top$. Hence,\n",
    "   $$\n",
    "    A + A^\\top = A + A = 2A.\n",
    "   $$\n",
    "   this simplifies to  \n",
    "   $$\n",
    "   2\\,X^\\top X\\,\\beta.\n",
    "   $$\n",
    "\n",
    "4. **Combine all pieces**  \n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\nabla_\\beta S\n",
    "   &= \\frac{\\partial}{\\partial\\beta}\\bigl(y^\\top y\\bigr)\n",
    "     + \\frac{\\partial}{\\partial\\beta}\\bigl(-2\\,\\beta^\\top X^\\top y\\bigr)\n",
    "     + \\frac{\\partial}{\\partial\\beta}\\bigl(\\beta^\\top X^\\top X\\,\\beta\\bigr) \\\\\n",
    "   &= 0 \\;-\\;2\\,X^\\top y \\;+\\;2\\,X^\\top X\\,\\beta.\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "Hence  \n",
    "$$\n",
    "\\boxed{\\nabla_\\beta S = -2\\,X^\\top y + 2\\,X^\\top X\\,\\beta.}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Set the gradient to zero → normal equations\n",
    "\n",
    "$$\n",
    "-2\\,X^\\top y + 2\\,X^\\top X\\,\\beta = 0\n",
    "\\quad\\Longrightarrow\\quad\n",
    "X^\\top X\\,\\beta = X^\\top y.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Closed-form solution\n",
    "\n",
    "Assuming $X^\\top X$ is invertible:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\hat\\beta = (X^\\top X)^{-1}\\,X^\\top y.\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Numeric example: detailed step-by-step\n",
    "\n",
    "Let \\(n=3\\), \\(p=2\\):\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "1 & 0 & 1\\\\\n",
    "1 & 1 & 0\\\\\n",
    "1 & 1 & 1\n",
    "\\end{pmatrix},\\quad\n",
    "y = \\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "### Step 1: Compute $X^\\top X$\n",
    "\n",
    "First write  \n",
    "$$\n",
    "X^\\top = \\begin{pmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "0 & 1 & 1\\\\\n",
    "1 & 0 & 1\n",
    "\\end{pmatrix}.\n",
    "$$  \n",
    "Then $(X^\\top X)_{ij} = \\sum_{k=1}^3 X_{k,i}\\,X_{k,j}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(X^\\top X)_{11} &= 1\\cdot1 + 1\\cdot1 + 1\\cdot1 = 3,\\\\\n",
    "(X^\\top X)_{12} &= 1\\cdot0 + 1\\cdot1 + 1\\cdot1 = 2,\\\\\n",
    "(X^\\top X)_{13} &= 1\\cdot1 + 1\\cdot0 + 1\\cdot1 = 2,\\\\\n",
    "(X^\\top X)_{21} &= 0\\cdot1 + 1\\cdot1 + 1\\cdot1 = 2,\\\\\n",
    "(X^\\top X)_{22} &= 0\\cdot0 + 1\\cdot1 + 1\\cdot1 = 2,\\\\\n",
    "(X^\\top X)_{23} &= 0\\cdot1 + 1\\cdot0 + 1\\cdot1 = 1,\\\\\n",
    "(X^\\top X)_{31} &= 1\\cdot1 + 0\\cdot1 + 1\\cdot1 = 2,\\\\\n",
    "(X^\\top X)_{32} &= 1\\cdot0 + 0\\cdot1 + 1\\cdot1 = 1,\\\\\n",
    "(X^\\top X)_{33} &= 1\\cdot1 + 0\\cdot0 + 1\\cdot1 = 2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "X^\\top X\n",
    "= \\begin{pmatrix}\n",
    "3 & 2 & 2\\\\\n",
    "2 & 2 & 1\\\\\n",
    "2 & 1 & 2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Compute $X^\\top y$\n",
    "\n",
    "Each entry is $\\sum_{k=1}^3 X_{k,i}\\,y_k$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(X^\\top y)_1 &= 1\\cdot1 + 1\\cdot2 + 1\\cdot3 = 6,\\\\\n",
    "(X^\\top y)_2 &= 0\\cdot1 + 1\\cdot2 + 1\\cdot3 = 5,\\\\\n",
    "(X^\\top y)_3 &= 1\\cdot1 + 0\\cdot2 + 1\\cdot3 = 4.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "X^\\top y = \\begin{pmatrix}6\\\\5\\\\4\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Invert $X^\\top X$\n",
    "\n",
    "1. **Determinant**  \n",
    "   $$\n",
    "   \\det(X^\\top X)\n",
    "   = 3\\begin{vmatrix}2&1\\\\1&2\\end{vmatrix}\n",
    "   - 2\\begin{vmatrix}2&1\\\\2&2\\end{vmatrix}\n",
    "   + 2\\begin{vmatrix}2&2\\\\2&1\\end{vmatrix}\n",
    "   = 3(4-1) - 2(4-2) + 2(2-4)\n",
    "   = 9 - 4 - 4\n",
    "   = 1.\n",
    "   $$\n",
    "\n",
    "2. **Cofactor matrix $C$**  \n",
    "   $$\n",
    "   C_{ij} = (-1)^{i+j}\\det\\bigl((X^\\top X)_{\\!-\\{i\\},-\\{j\\}}\\bigr)\n",
    "   \\quad\\Longrightarrow\\quad\n",
    "   C = \\begin{pmatrix}\n",
    "     3 & -2 & -2\\\\\n",
    "    -2 &  2 &  1\\\\\n",
    "    -2 &  1 &  2\n",
    "   \\end{pmatrix}.\n",
    "   $$\n",
    "\n",
    "3. **Adjugate = $C^\\top$** (here symmetric)  \n",
    "4. **Inverse**  \n",
    "   $$\n",
    "   (X^\\top X)^{-1}\n",
    "   = \\frac{1}{\\det(X^\\top X)}\\,\\operatorname{adj}(X^\\top X)\n",
    "   = C^\\top\n",
    "   = \\begin{pmatrix}\n",
    "     3 & -2 & -2\\\\\n",
    "    -2 &  2 &  1\\\\\n",
    "    -2 &  1 &  2\n",
    "   \\end{pmatrix}.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Solve for $\\hat\\beta$\n",
    "\n",
    "Multiply:\n",
    "\n",
    "$$\n",
    "\\hat\\beta\n",
    "= (X^\\top X)^{-1}\\,X^\\top y\n",
    "= \\begin{pmatrix}\n",
    " 3 & -2 & -2\\\\\n",
    "-2 &  2 &  1\\\\\n",
    "-2 &  1 &  2\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}6\\\\5\\\\4\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "3\\cdot6 +(-2)\\cdot5 +(-2)\\cdot4\\\\\n",
    "(-2)\\cdot6 + 2\\cdot5 +1\\cdot4\\\\\n",
    "(-2)\\cdot6 + 1\\cdot5 +2\\cdot4\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}0\\\\2\\\\1\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "So $\\hat\\beta_0=0,\\;\\hat\\beta_1=2,\\;\\hat\\beta_2=1$, giving\n",
    "\n",
    "$$\n",
    "\\hat y = 0 + 2\\,x_1 + 1\\,x_2,\n",
    "$$\n",
    "\n",
    "which exactly fits all three observations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
